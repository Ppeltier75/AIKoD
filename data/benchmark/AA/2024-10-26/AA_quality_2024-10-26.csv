model_name,chatbot_arena_elo,quality_index,mmlu,gpqa,humaneval,math,mgsm
o1-preview,,84.600,0.908,0.669,0.954,0.855,0.908
o1-mini,1314.000,81.600,0.852,0.578,0.934,0.900,0.899
GPT-4o (Aug '24),1337.000,77.200,0.886,0.514,0.905,0.783,0.901
GPT-4o (May '24),1285.000,76.700,0.872,0.507,0.922,0.766,0.899
GPT-4o mini,1272.000,71.400,0.819,0.427,0.856,0.753,0.870
Llama 3.1 Instruct 405B,1266.000,71.900,0.868,0.500,0.822,0.688,0.831
Llama 3.2 Instruct 90B (Vision),,66.400,0.841,0.428,0.747,0.639,
Llama 3.1 Instruct 70B,1249.000,65.400,0.835,0.436,0.746,0.601,0.829
Llama 3.2 Instruct 11B (Vision),,53.600,0.716,0.259,0.671,0.501,
Llama 3.1 Instruct 8B,1172.000,53.000,0.711,0.268,0.640,0.500,0.675
Llama 3.2 Instruct 3B,,46.700,0.638,0.210,0.518,0.503,
Llama 3.2 Instruct 1B,,27.100,0.350,0.136,0.348,0.250,
Gemini 1.5 Pro (Sep '24),1299.000,79.700,0.861,0.607,0.871,0.850,
Gemini 1.5 Flash (Sep '24),1269.000,73.400,0.807,0.499,0.837,0.794,
Gemma 2 27B,1212.000,61.400,0.765,0.385,0.744,0.563,0.831
Gemma 2 9B,1185.000,46.400,0.732,0.308,0.282,0.533,0.787
Gemini 1.5 Flash (May '24),1269.000,,0.789,0.390,,0.550,0.756
Gemini 1.5 Pro (May '24),1299.000,,0.860,0.460,,0.680,0.763
Gemini 1.5 Flash-8B,,,0.750,0.298,,0.699,
Claude 3.5 Sonnet (June '24),1269.000,76.900,0.881,0.559,0.898,0.738,0.918
Claude 3 Opus,1248.000,70.300,0.841,0.497,0.841,0.632,0.897
Claude 3 Haiku,1179.000,54.200,0.708,0.333,0.717,0.410,0.713
Mistral Large 2,1251.000,73.000,0.846,0.484,0.871,0.720,0.874
Mixtral 8x22B Instruct,1148.000,61.200,0.763,0.367,0.698,0.619,0.601
Mistral Small (Sep '24),,60.400,0.741,0.338,0.734,0.602,0.676
Pixtral 12B (2409),,56.300,0.695,0.304,0.715,0.537,
Ministral 8B,,53.300,0.589,0.299,0.744,0.500,
Mistral NeMo,,51.900,0.661,0.330,0.682,0.403,0.486
Ministral 3B,,50.800,0.579,0.261,0.713,0.478,
Mixtral 8x7B Instruct,1114.000,42.000,0.633,0.305,0.409,0.333,0.300
Codestral-Mamba,,35.500,0.254,0.028,0.782,0.356,0.340
Command-R+ (Aug '24),,55.900,0.750,0.339,0.710,0.436,0.663
Command-R (Aug '24),,51.100,0.674,0.270,0.695,0.404,0.575
Command-R+ (Apr '24),1190.000,45.900,0.682,0.236,0.636,0.282,0.555
Command-R (Mar '24),1149.000,35.900,0.591,0.258,0.435,0.152,0.298
Sonar 3.1 Large,,,,,,,
Sonar 3.1 Small,,,,,,,
Phi-3 Medium Instruct 14B,1123.000,,,,,,
Solar Pro,,60.500,0.796,0.376,0.726,0.522,
Solar Mini,,47.500,0.660,0.284,0.617,0.340,
DBRX Instruct,1103.000,48.800,0.702,0.309,0.584,0.356,0.557
Llama 3.1 Nemotron Instruct 70B,,69.900,0.856,0.477,0.751,0.711,
Reka Flash (Sep '24),,57.900,0.731,0.340,0.707,0.540,
Reka Core,1200.000,56.800,0.760,0.278,0.685,0.550,0.747
Reka Flash (Feb '24),,46.200,0.646,0.267,0.568,0.367,0.510
Reka Edge,,29.600,0.437,0.193,0.376,0.178,0.323
Jamba 1.5 Large,,64.000,0.804,0.410,0.741,0.603,0.741
Jamba 1.5 Mini,,45.600,0.634,0.260,0.610,0.321,0.296
DeepSeek-Coder-V2,,66.500,0.797,0.421,0.857,0.584,0.852
DeepSeek-V2-Chat,1219.000,65.900,0.801,0.418,0.776,0.641,0.860
DeepSeek-V2.5,,65.800,0.805,0.418,0.857,0.553,0.857
Qwen2.5 Instruct 72B,,75.200,0.857,0.496,0.846,0.811,
Qwen2 Instruct 72B,1187.000,69.000,0.826,0.401,0.792,0.742,0.810
Yi-Large,1212.000,58.300,0.779,0.335,0.682,0.538,0.723
GPT-4 Turbo,1257.000,74.300,0.865,0.497,0.872,0.736,0.900
GPT-3.5 Turbo,1107.000,51.600,0.677,0.302,0.693,0.394,0.521
GPT-3.5 Turbo Instruct,,,,,,,
GPT-4,1186.000,,,,,,
Llama 3 Instruct 70B,1206.000,62.000,0.794,0.392,0.770,0.525,0.823
Llama 3 Instruct 8B,1152.000,45.900,0.643,0.304,0.588,0.302,0.551
Llama 2 Chat 70B,1093.000,38.600,0.602,0.253,0.464,0.224,0.396
Llama 2 Chat 13B,1063.000,35.600,0.546,0.247,0.412,0.218,0.346
Llama 2 Chat 7B,1037.000,,0.125,0.063,0.134,,
Gemini 1.0 Pro,1111.000,,,,,,0.645
Claude 3 Sonnet,1201.000,57.200,0.765,0.373,0.694,0.456,0.840
Mistral Large,1157.000,56.100,0.687,0.359,0.690,0.508,0.608
Mistral Small (Feb '24),,49.900,0.690,0.314,0.537,0.454,0.589
Mistral 7B Instruct,1008.000,24.400,0.332,0.186,0.318,0.139,0.229
Mistral Medium,1148.000,,,,,,
Codestral,,,,0.229,0.777,,
OpenChat 3.5 (1210),1076.000,42.500,0.559,0.224,0.637,0.280,0.387
Jamba Instruct,,28.200,0.577,0.252,0.002,0.296,0.375
Claude 3.5 Sonnet (Oct '24),,80.000,0.895,0.582,0.932,0.790,
