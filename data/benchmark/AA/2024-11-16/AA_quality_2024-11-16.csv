model_name,chatbot_arena_elo,quality_index,mmlu,gpqa,humaneval,math,mgsm,median_output_tokens_per_second
o1-preview,,84.600,0.908,0.669,0.954,0.855,0.908,34.5432379040835
o1-mini,1314.000,81.600,0.852,0.578,0.934,0.900,0.899,72.9896230318334
GPT-4o (Aug '24),1337.000,77.200,0.886,0.514,0.905,0.783,0.901,82.104636395751
GPT-4o (May '24),1285.000,76.700,0.872,0.507,0.922,0.766,0.899,88.1731502197295
GPT-4o mini,1272.000,71.400,0.819,0.427,0.856,0.753,0.870,98.9744328344844
Llama 3.1 Instruct 405B,1266.000,72.100,0.868,0.503,0.824,0.688,0.834,74.9755235121744
Llama 3.2 Instruct 90B (Vision),,66.700,0.840,0.415,0.750,0.664,,41.2591831688338
Llama 3.1 Instruct 70B,1249.000,65.400,0.835,0.434,0.748,0.600,0.831,71.0597737321562
Llama 3.2 Instruct 11B (Vision),,53.400,0.714,0.247,0.675,0.502,,128.821636461976
Llama 3.1 Instruct 8B,1172.000,53.000,0.711,0.268,0.640,0.501,0.676,156.990359949986
Llama 3.2 Instruct 3B,,46.700,0.638,0.210,0.518,0.503,,201.997397704461
Llama 3.2 Instruct 1B,,27.100,0.350,0.136,0.348,0.250,,552.522318155649
Gemini 1.5 Pro (Sep '24),1299.000,79.700,0.861,0.607,0.871,0.850,,59.6708292026659
Gemini 1.5 Flash (Sep '24),1269.000,73.400,0.807,0.499,0.837,0.794,,198.625850245653
Gemma 2 27B,1212.000,61.400,0.765,0.385,0.744,0.563,0.831,41.875167216976
Gemma 2 9B,1185.000,46.400,0.732,0.308,0.282,0.533,0.787,132.351379669829
Gemini 1.5 Flash-8B,,,0.750,0.298,,0.699,,282.692482840685
Gemini 1.5 Pro (May '24),1299.000,,0.860,0.460,,0.680,0.763,63.2712615067695
Gemini 1.5 Flash (May '24),1269.000,,0.789,0.390,,0.550,0.756,312.78404231882
Claude 3.5 Sonnet (Oct '24),,80.000,0.895,0.582,0.932,0.790,,54.8459692347041
Claude 3.5 Sonnet (June '24),1269.000,76.900,0.881,0.559,0.898,0.738,0.918,55.0584685939263
Claude 3 Opus,1248.000,70.300,0.841,0.497,0.841,0.632,0.897,26.727769264977
Claude 3.5 Haiku,,69.000,0.810,0.369,0.854,0.730,,63.6354551391265
Claude 3 Haiku,1179.000,54.200,0.708,0.333,0.717,0.410,0.713,127.509844401348
Mistral Large 2,1251.000,73.000,0.846,0.484,0.871,0.720,0.874,35.017812478403
Mixtral 8x22B Instruct,1148.000,61.000,0.761,0.361,0.696,0.623,0.600,77.0149880925665
Mistral Small (Sep '24),,60.400,0.741,0.338,0.734,0.602,0.676,59.129824587641
Pixtral 12B (2409),,56.300,0.695,0.304,0.715,0.537,,69.1750492767835
Ministral 8B,,53.300,0.589,0.299,0.744,0.500,,135.370569253388
Mistral NeMo,,51.900,0.661,0.330,0.682,0.403,0.486,69.7110747623256
Ministral 3B,,50.800,0.579,0.261,0.713,0.478,,207.443527156857
Mixtral 8x7B Instruct,1114.000,42.700,0.642,0.304,0.430,0.330,0.306,89.4207176992936
Codestral-Mamba,,35.500,0.254,0.028,0.782,0.356,0.340,94.3401877431915
Command-R+ (Aug '24),,55.900,0.750,0.339,0.710,0.436,0.663,49.8680864178212
Command-R (Aug '24),,51.100,0.674,0.270,0.695,0.404,0.575,110.032634148022
Command-R+ (Apr '24),1190.000,45.900,0.682,0.236,0.636,0.282,0.555,46.9321999971985
Command-R (Mar '24),1149.000,35.900,0.591,0.258,0.435,0.152,0.298,108.402097204465
Aya Expanse 8B,,,,,,,,137.684899527401
Aya Expanse 32B,,,,,,,,121.248051101857
Sonar 3.1 Small,,,,,,,,147.109818716845
Sonar 3.1 Large,,,,,,,,56.4606024092636
Grok Beta,,70.400,0.847,0.429,0.849,0.692,,56.2760167132491
Phi-3 Medium Instruct 14B,1123.000,,,,,,,44.1799182011055
Solar Pro,,60.500,0.796,0.376,0.726,0.522,,47.6411804487861
Solar Mini,,47.500,0.660,0.284,0.617,0.340,,84.1196152793561
DBRX Instruct,1103.000,48.800,0.702,0.309,0.584,0.356,0.557,83.0090847810396
Llama 3.1 Nemotron Instruct 70B,,69.900,0.856,0.477,0.751,0.711,,27.9302109481308
Reka Flash (Sep '24),,57.900,0.731,0.340,0.707,0.540,,32.8076050711904
Reka Core,1200.000,56.800,0.760,0.278,0.685,0.550,0.747,14.7615608021287
Reka Flash (Feb '24),,46.200,0.646,0.267,0.568,0.367,0.510,31.2056851764366
Reka Edge,,29.600,0.437,0.193,0.376,0.178,0.323,35.1956163232453
Jamba 1.5 Large,,64.000,0.804,0.410,0.741,0.603,0.741,51.0652203449513
Jamba 1.5 Mini,,45.600,0.634,0.260,0.610,0.321,0.296,82.6360186722809
DeepSeek-Coder-V2,,66.500,0.797,0.421,0.857,0.584,0.852,16.3954222694204
DeepSeek-V2-Chat,1219.000,65.900,0.801,0.418,0.776,0.641,0.860,16.5737958591432
DeepSeek-V2.5,,65.800,0.805,0.418,0.857,0.553,0.857,13.5374398875752
Qwen2.5 Instruct 72B,,75.200,0.857,0.496,0.846,0.811,,46.9962385146722
Qwen2.5 Coder 32B Instruct,,70.100,0.788,0.408,0.890,0.719,,36.9857687554267
Qwen2 Instruct 72B,1187.000,69.000,0.826,0.401,0.792,0.742,0.810,54.7823877084047
Yi-Large,1212.000,58.300,0.779,0.335,0.682,0.538,0.723,67.589807910948
GPT-4 Turbo,1257.000,74.300,0.865,0.497,0.872,0.736,0.900,35.7885663785505
GPT-3.5 Turbo,1107.000,51.600,0.677,0.302,0.693,0.394,0.521,100.476576678239
GPT-4,1186.000,,,,,,,23.3512764768917
GPT-3.5 Turbo Instruct,,,,,,,,108.774945219725
Llama 3 Instruct 70B,1206.000,61.900,0.794,0.390,0.768,0.525,0.823,45.1697889548425
Llama 3 Instruct 8B,1152.000,45.900,0.643,0.304,0.588,0.302,0.551,119.421964553143
Llama 2 Chat 13B,1063.000,24.800,0.449,0.193,0.213,0.135,0.150,53.2655071995605
Llama 2 Chat 7B,1037.000,,0.125,0.063,0.134,,,123.791851881479
Gemini 1.0 Pro,1111.000,,,,,,0.645,102.004574633893
Claude 3 Sonnet,1201.000,57.200,0.765,0.373,0.694,0.456,0.840,62.7209960786006
Mistral Large,1157.000,56.100,0.687,0.359,0.690,0.508,0.608,35.6290136030955
Mistral Small (Feb '24),,49.900,0.690,0.314,0.537,0.454,0.589,53.688742560349
Mistral 7B Instruct,1008.000,24.400,0.336,0.188,0.314,0.139,0.197,96.1338243372718
Mistral Medium,1148.000,,,,,,,44.2363634406167
Codestral,,,,0.229,0.777,,,80.8582724411556
OpenChat 3.5 (1210),1076.000,42.500,0.559,0.224,0.637,0.280,0.387,74.8673257127849
Jamba Instruct,,28.200,0.577,0.252,0.002,0.296,0.375,75.8126845655275
